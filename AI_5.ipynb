{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各種ライブラリが必要となるため、プログラム実行時はインストールすること\n",
    "\n",
    "py -m pip install <ライブラリ>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前準備\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# 可視化ライブラリ\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "# 少数第3位まで表示\n",
    "%precision 3\n",
    "\n",
    "# グラフの日本語表記対応\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第５章 ディープラーニングの概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-1 ディープラーニングの特徴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークは、機械学習の手法の1つとみなす。\n",
    "\n",
    "人間の脳神経回路を模したニューラルネットワークを多層的にすることにより、複雑な関数を近似できるようになり、コンピューター自身がデータに含まれる特徴を捉えられるようになった。これにより、ニューラルネットワークは、ディープラーニングと呼ばれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの学習により観測データから本質的な情報を抽出した特徴のことを**内部表現**という。その結果、ニューラルネットワークは特徴量の設計とその後の処理をまとめて自動的に行うことができるという**エンドツーエンド学習**ができるようになった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-2 多層パーセプトロン"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多層パーセプトロンは、最も基本でよくつかわれるニューラルネットワークである。層状に**ユニットニューロン**が並び、隣接する層の間のみで結合している。\n",
    "\n",
    "入力層⇒中間層⇒出力層という流れで処理する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ユニットは複数の入力を受け取り、1つの値を出力する。\n",
    "\n",
    "各入力にそれぞれ**重み**をかけたものの総和に、**バイアス**を足した値を出力する。この出力した値を**活性化関数**によって変換されて、次の層のユニットの入力になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2-1 活性化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数は、ユニットにおける入力の総和を出力に変換する関数のことである。出力層と中間層で使われている。\n",
    "\n",
    "例えば、出力層の活性化関数には、以下がある。\n",
    "\n",
    "* 単純パーセプトロンでは**ステップ関数**が、回帰では**恒等関数**が一般的に利用される。\n",
    "* 他クラス分類における出力層では、ソフトマックス関数が良く利用される\n",
    "\n",
    "中間層の活性化関数には、以下がある。\n",
    "\n",
    "* tanh関数(双曲線正接関数)、**シグモイド関数**などが利用される<br>tanh関数は-1から1までの値をとり、微分の最大値が1になり、シグモイド関数の0.25より大きいことから勾配消失が起こりにくい\n",
    "* さらに勾配消失が起きにくいように改善された**ReLU**や**Maxout**などが利用される<br>ReLUは、入力が0を超えている場合はそのまま出力し、0未満の場合は0とする関数<br>Maxoutは、複数の線形関数を1つにした構造を持ち、それらの中での最大値を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2-2 誤差関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差関数は、正解ラベルと入力データを多層パーセプトロンに入力し得られる出力との近さを評価する。誤差関数から得られる誤差をできるだけ小さくする重みを求めることが学習の目的となる。\n",
    "\n",
    "|問題|よく利用される関数|\n",
    "|---|---|\n",
    "|回帰問題|平均二乗誤差関数|\n",
    "|他クラス分類|交差エントロピー誤差関数|\n",
    "\n",
    "また、自己符号化器において入力と出力の2つの分布から学習する時には、KLダイバージェンスが用いられることもある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-3 確率的再急降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3-1 確率的最急降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの学習目的は、**誤差関数**の出力をできるだけ小さくする重みやバイアスなどのパラメータを見つけることである。この問題を解くことを最適化といい、このパラメータを見つけるアルゴリズムとして**勾配降下法**(GD)という。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配降下法は、重みと誤差関数を重みで微分して得られる勾配$\\nabla E$を用いて、重みを更新する。現在の重み$w^{(t)}$を、更新した後の重みを$w^{(t+1)}$とすると、以下の式を繰り返し計算することで最適解に田とりつくことができる。\n",
    "\n",
    "$$\n",
    "w^{(t+1)} = w^{(t)} - \\epsilon \\nabla E\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この手法は、1回の重みの更新ですべてのデータを利用することから**バッチ学習**と呼ぶ。ここで、$\\epsilon$は$w$の更新の量を決める数で、**学習率**と呼ぶ。\n",
    "\n",
    "この学習率は、重みの更新方法に大きな影響を与える。\n",
    "\n",
    "|学習率|メリット|デメリット|\n",
    "|---|---|---|\n",
    "|大きい|収束が早く、学習時間が短い|誤差が大きくなる傾向|\n",
    "|小さい|収束が遅く、学習時間が長い|誤差が小さくなる傾向|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークにおける学習の手順は以下となる。\n",
    "\n",
    "1. 重みとバイアスを初期化\n",
    "1. データをニューラルネットワークに入力し、その結果を出力\n",
    "1. 誤差関数によって、ニューラルネットワークの出力と正解ラベルとの誤差を計算\n",
    "1. 誤差をより小さくするように勾配降下法によって重みとバイアスを更新\n",
    "1. 上記の2～4の手順を何度も繰り返して最適な重みとバイアスに近づける"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークでは、勾配降下法の中の確率的勾配降下法(SGD)などが良く利用される。主に、オンライン学習やミニバッチ学習を用いて勾配降下法をことをいう。\n",
    "\n",
    "|学習方法|説明|\n",
    "|---|---|\n",
    "|バッチ学習|1回の重みの更新ですべてのデータを利用する方法|\n",
    "|オンライン学習|ランダムに抽出した1つのサンプルだけを使用しパラメータを更新する方法|\n",
    "|ミニバッチ学習|一定数のサンプルをランダムに抽出し利用してパラメータを更新する方法|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3-2 過学習を防ぐテクニック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークは高い表現力を持つ一方、過学習を起こしやすいという問題がある。この過学習を防ぐテクニックを以下にまとめる。\n",
    "\n",
    "|手法|説明|\n",
    "|---|---|\n",
    "|ドロップアウト|学習する際、層の中のノードの内のいくつかを無効にして学習を行い、<br>次の更新では別のノードを無効にして学習を行うことを繰り返す手法|\n",
    "|正則化|誤差関数に正則化項を加えることによって過学習を防ぐ手法<br>重みが減衰しやすいことから、荷重減衰と呼ぶ<br>・L1ノルムを用いる手法：**L1正則化**（回帰利用時、Lasso回帰）<br>・L2ノルムを用いる手法：**L2正則化**（回帰利用時、Rigde回帰）|\n",
    "|バッチ正規化|ニューラルネットワーク内のユニットの出力値を正規化する手法<br>入力の分布が学習途中で大きく変わる**内部共変量シフト**を防ぐことが可能|\n",
    "|データ拡張|手持ちのデータになんらかの加工を行って、量を水増しすること|\n",
    "|初期停止|テストデータに対する誤差であるテスト誤差を最小にするのが目的<br>パラメータの更新によってテスト誤差が大きくなるような状況では、その時点で学習を終了する|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3-3 学習のテクニック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データには偏りがある可能性があるため、偏りがないようにデータを変換する前処理が必要となる。例えば、平均と分散をそろえることである。また、学習を始める前に初期化する。最も一般的な方法は、ガウス分布からランダムにあたうぃお抽出する方法である。また、直前の層のノード数が$n$の場合、初期値として$1 / \\sqrt{n}$を標準偏差として分布を使う**Xavierの初期値**がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配降下法において、学習率の大きさは重要である。学習率が低いと収束が遅くなったり、学習に時間がかかったり、**局所最適解**へ収束する。\n",
    "\n",
    "そのため、様々な手法がある。\n",
    "\n",
    "|手法|説明|\n",
    "|---|---|\n",
    "|AdaGrad|学習が進むにつれて学習率を自動的に小さくする|\n",
    "|RMSprop|AdaGradが急速に学習率が低下するという問題を解決<br>勾配の2乗の指数移動を取るように変更された|\n",
    "|AdaDelta|RMSpropと同様に、AdaGradの発展形<br>過去すべての勾配の2乗を蓄積しておくのではなく、過去の勾配を蓄積する範囲を制限する|\n",
    "|モメンタム|運動量(モメンタム)と呼ばれる物理量を用いて過去の重みの更新量を考慮して、重みの更新を行う|\n",
    "|Adam|過去の勾配の2乗の指数移動平均を用いることで、勾配の平均と分散を推定|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークには学習率や正則化係数などの多くのハイパーパラメータがあり、これらが精度に大きな影響を与える。ハイパーパラメータのチューニング手法は以下がある。\n",
    "\n",
    "|方法|説明|\n",
    "|---|---|\n",
    "|グリッドサーチ|設定した範囲においてすべてのパラメータの組み合わせを試してみる方法|\n",
    "|ベイズ最適化|比較的少ない試行回数でより優れたハイパーパラメータを選ぶことが可能|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-4 ニューラルネットワークの歴史"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4-1 ニューラルネットワークの歴史"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|ブーム|年代|概要|衰退理由|\n",
    "|---|---|---|---|\n",
    "|第1次ニューロブーム|1958年|パーセプトロン(入力と出力層)により学習と予測ができるようになった|排他的論理和(XOR)の学習ができないことでブームが終わった|\n",
    "|第2次ニューロブーム|1986年|パーセプトロンに隠れ層を追加することでXOR問題が解消<br>隠れ層を持つニューラルネットを高速に学習させる誤差逆伝播法が発表された|多層パーセプトロンにおいて学習できない<br>勾配消失問題が起こったり、適切な重みの初期値を決めることが<br>難しく過学習起こしたりした<br>十分なデータが得られなかった|\n",
    "|第3次ニューロブーム|2000年|以下が充実・性能が上がったため、ニューラルネットワークの多層化が実現した<br>・データセット<br>・ハードウェア<br>・フレームワーク|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4-2 ディープラーニングが普及した理由"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "よく利用されるデータセットを以下にまとめる。\n",
    "\n",
    "|データセット|説明|\n",
    "|---|---|\n",
    "|MNIST|アメリカの国立標準技術研究所が提供する手書き数字の画像データ|\n",
    "|ImageNet|スタンフォード大学がインターアネット上から画像を集めて分類したデータ<br>2万種類以上の物体名と1400万枚の画像を収録|\n",
    "|Youtube-8M|4800件のラベルでタグ付けされた800万本のYouTube動画データ|\n",
    "|CIFAR-100|100種類の画像をそれぞれ600枚ずつ、合計60000枚収録したデータ|\n",
    "|ILSVRC2012|2012年の画像認識コンペティションで利用したImageNetの中から抽出されたデータ|\n",
    "|Caltech-256|Google画像検索でダウンロードして手作業で256カテゴリに分類された30607画像のデータ|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-4-2-2 ハードウェア"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ディープニューラルネットワークの学習では高速な処理を実現できるハードウェアが必要となる。そのため、TensorFlowの前身の分散並列技術である**DistBelief**や、リアルタイムに画像処理を行うことに特化した演算装置である**GPU**(並列演算の性能がよい)が利用されてきた。\n",
    "\n",
    "GPGPUによって、GPUの機能を画像以外にも転用できるようになった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-4-2-3 フレームワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フレームワークは、ニューラルネットワークモデルを定義し、データを用いて学習・予測を実行する。フレームワークによってネットワークの記述方法が異なる。\n",
    "\n",
    "|記述方法|特徴|フレームワーク|\n",
    "|---|---|---|\n",
    "|設定ファイルのみ配置|簡単に学習を始められるが、複雑なモデルを記述することが難しい|Caffe、CNTK|\n",
    "|プログラム実装|実装により複雑なモデルを記述可能|TensorFlow、Chainer、PyTorch|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
